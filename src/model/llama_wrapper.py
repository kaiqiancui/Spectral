import torch
import torch.nn as nn
import os
from transformers import AutoModelForCausalLM, AutoTokenizer

class LlamaWrapper(nn.Module):
    def __init__(self, config, projector):
        super().__init__()
        self.cfg = config
        model_path = config.llm.model_path
        
        print(f"ğŸ¤– Loading Llama-3.1 (Inference Mode)...")
        self.llm = AutoModelForCausalLM.from_pretrained(
            model_path,
            device_map=config.experiment.device,
            load_in_4bit=config.llm.load_in_4bit,
            torch_dtype=torch.float16,
            trust_remote_code=True
        )
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.tokenizer.pad_token = self.tokenizer.eos_token
        self.tokenizer.padding_side = "left"
        
        # æ·»åŠ å ä½ç¬¦ <REP>
        self.rep_token = "<REP>"
        if self.rep_token not in self.tokenizer.get_vocab():
            self.tokenizer.add_tokens([self.rep_token], special_tokens=True)
            self.llm.resize_token_embeddings(len(self.tokenizer))
        self.rep_token_id = self.tokenizer.convert_tokens_to_ids(self.rep_token)
        
        # éšæœºæŠ•å½±å±‚ (ä¸å‚ä¸æ¢¯åº¦æ›´æ–°)
        self.projector = projector
        for p in self.projector.parameters():
            p.requires_grad = False

        # ç³»ç»Ÿæç¤ºè¯
        prompt_path = config.llm.get("prompt_file", None)
        if prompt_path and os.path.exists(prompt_path):
            with open(prompt_path, "r") as f:
                self.system_message = f.read().strip()
        else:
            self.system_message = "Predict the value based on the representation."

    def apply_alignment(self, embs, align_stats):
        """
        æ ¸å¿ƒé€»è¾‘ï¼šç»Ÿè®¡å¯¹é½ (Alignment)
        å…¬å¼: (x - mu_x) / std_x * std_tgt + mu_tgt
        """
        if align_stats is None:
            return embs
            
        mu_x = embs.mean(dim=0, keepdim=True)
        std_x = embs.std(dim=0, keepdim=True) + 1e-8
        
        # ç›®æ ‡åˆ†å¸ƒ (LLM çš„ Embedding åˆ†å¸ƒ)
        mu_tgt = align_stats['target_mean'].to(embs.device)
        std_tgt = align_stats['target_std'].to(embs.device)
        
        return (embs - mu_x) / std_x * std_tgt + mu_tgt

    def _build_prompt(self, text, label=None, is_shot=False):
        # è¯»å– Config ä¸­çš„æ¨¡æ¿ï¼Œè€Œä¸æ˜¯ç¡¬ç¼–ç 
        # å‡è®¾æ¨¡æ¿æ˜¯: "The molecule is <REP>. Property is:"
        template = self.cfg.llm.prompt_template 
        
        # æ›¿æ¢ <REP> å ä½ç¬¦
        prompt = template.replace("<REP>", self.rep_token)
        prompt = prompt.replace("<SMILES>", text) # å¦‚æœæ¨¡æ¿é‡Œæœ‰åŸæ–‡å ä½ç¬¦
        
        if is_shot:
            prompt += f" {label:.3f}\n" # Shot ç»“å°¾åŠ  Label
        else:
            prompt += "" # Query ç»“å°¾ç•™ç©ºè®© LLM ç»­å†™
            
        return prompt

    @torch.inference_mode()
    def generate(self, net_input, align_stats=None):
        """
        Training-Free æ¨ç†å‡½æ•°
        align_stats: ä»è®­ç»ƒé›†è®¡ç®—å‡ºçš„å¯¹é½å‚æ•°
        """
        query_texts = net_input['query_text']
        query_embs = net_input['query_emb']
        shots_batch = net_input['shots']
        
        batch_size = len(query_texts)
        full_prompts = []
        all_reps_list = []
        
        # 1. æ‹¼æ¥ Prompt å’Œ æ”¶é›† Embeddings
        sys_header = f"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\n{self.system_message}<|eot_id|>"
        
        for i in range(batch_size):
            curr_prompt = sys_header
            curr_reps = []
            
            # Shots
            for shot in shots_batch[i]:
                curr_prompt += self._build_prompt(shot['text'], shot['label'], is_shot=True)
                curr_reps.append(shot['emb'])
            
            # Query
            curr_prompt += self._build_prompt(query_texts[i], label=None, is_shot=False)
            curr_reps.append(query_embs[i])
            
            full_prompts.append(curr_prompt)
            all_reps_list.append(torch.stack(curr_reps))
            
        # 2. Tokenize
        inputs = self.tokenizer(full_prompts, return_tensors="pt", padding=True, truncation=True, max_length=2048).to(self.llm.device)
        
        # 3. æŠ•å½± + å¯¹é½ (Projection + Alignment)
        flat_reps = torch.cat(all_reps_list, dim=0).to(self.llm.device).to(self.llm.dtype)
        
        # Step A: éšæœºæŠ•å½± (640 -> 4096)
        projected_reps = self.projector(flat_reps)
        
        # Step B: ç»Ÿè®¡å¯¹é½ (å…³é”®æ­¥éª¤!)
        # è®©æŠ•å½±åçš„å‘é‡åˆ†å¸ƒ çœ‹èµ·æ¥åƒ Llama çš„ Token Embedding
        aligned_reps = self.apply_alignment(projected_reps, align_stats)
        
        # 4. æ›¿æ¢ Embedding
        inputs_embeds = self.llm.get_input_embeddings()(inputs.input_ids)
        is_rep_token = (inputs.input_ids == self.rep_token_id)
        
        if is_rep_token.sum() == aligned_reps.shape[0]:
            inputs_embeds[is_rep_token] = aligned_reps
        else:
            # æˆªæ–­ä¿æŠ¤
            min_len = min(is_rep_token.sum(), aligned_reps.shape[0])
            inputs_embeds[is_rep_token] = aligned_reps[:min_len]

        # 5. Generate (ç›´æ¥ç”Ÿæˆæ–‡æœ¬)
        outputs = self.llm.generate(
            inputs_embeds=inputs_embeds,
            attention_mask=inputs.attention_mask,
            max_new_tokens=10,
            pad_token_id=self.tokenizer.eos_token_id,
            do_sample=False, # ç¡®å®šæ€§ç”Ÿæˆ
            temperature=None
        )
        
        # è§£ç ç”Ÿæˆçš„æ–‡æœ¬
        decoded_output = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)
        return decoded_output