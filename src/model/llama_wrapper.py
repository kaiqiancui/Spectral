import torch
import torch.nn as nn
from transformers import AutoModelForCausalLM, AutoTokenizer

class LlamaWrapper(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.cfg = config
        model_path = config.llm.model_path
        
        print(f"ğŸ¤– Loading LLM: {model_path}...")
        self.llm = AutoModelForCausalLM.from_pretrained(
            model_path,
            device_map=config.experiment.device,
            torch_dtype=torch.float16,
            trust_remote_code=True
        )
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.tokenizer.pad_token = self.tokenizer.eos_token
        self.tokenizer.padding_side = "left"
        
        # 1. å®šä¹‰å ä½ç¬¦ Token (ç”¨äºå®šä½ Embedding æ’å…¥ç‚¹)
        # æˆ‘ä»¬ä½¿ç”¨ <REP> ä½œä¸ºé€šç”¨å ä½ç¬¦
        self.rep_token = "<REP>"
        if self.rep_token not in self.tokenizer.get_vocab():
            self.tokenizer.add_tokens([self.rep_token], special_tokens=True)
            self.llm.resize_token_embeddings(len(self.tokenizer))
        self.rep_token_id = self.tokenizer.convert_tokens_to_ids(self.rep_token)
        
        # 2. åŠ è½½ Prompt æ¨¡æ¿
        # ç¤ºä¾‹ Template: "Question: What is the property of <REP>? Answer:"
        self.prompt_template = config.llm.get("prompt_template", "Input: <REP>\nOutput:")

    def _build_prompt_text(self, text_data):
        """
        æ ¹æ® Config çš„æ¨¡æ¿æ„å»ºæ–‡æœ¬ Prompt
        text_data: åŒ…å« 'input1', 'input2' ç­‰åŸå§‹æ–‡æœ¬çš„å­—å…¸
        """
        prompt = self.prompt_template
        
        # ç®€å•æ›¿æ¢ï¼šå¦‚æœæ¨¡æ¿é‡Œæœ‰ <INPUT> ä¹‹ç±»çš„æ ‡ç­¾ï¼Œå¯ä»¥ç”¨ text_data æ›¿æ¢
        # è¿™é‡Œå‡è®¾æ¨¡æ¿ä¸»è¦æ˜¯ä¸ºäº†å®‰æ”¾ <REP>
        # å¦‚æœæ˜¯ DTI ä»»åŠ¡ï¼Œæ¨¡æ¿å¯èƒ½æ˜¯ "Drug: <REP> Target: <REP> ..."
        # LlamaWrapper ä¸éœ€è¦çŸ¥é“æ˜¯ Drug è¿˜æ˜¯ Targetï¼Œå®ƒåªè´Ÿè´£çœ‹åˆ°ä¸€ä¸ª <REP> å°±å‡†å¤‡å¡«ä¸€ä¸ªå‘é‡
        return prompt

    def forward(self, batch):
        # ä»…ç”¨äºè®­ç»ƒæˆ–è°ƒè¯•ï¼Œé€šå¸¸æˆ‘ä»¬ç”¨ generate
        pass

    @torch.inference_mode()
    def generate(self, batch):
        """
        æ‰§è¡Œæ¨ç†
        batch: DataLoader yield å‡ºçš„å­—å…¸
        """
        device = self.llm.device
        batch_size = len(batch['input1']) # å‡è®¾ batch åŒ…å« input1, input1_emb ç­‰
        
        # 1. æ„å»ºçº¯æ–‡æœ¬ Prompt List
        prompts = [self._build_prompt_text(None) for _ in range(batch_size)]
        
        # 2. Tokenize
        inputs = self.tokenizer(prompts, return_tensors="pt", padding=True, truncation=True).to(device)
        input_ids = inputs.input_ids
        attention_mask = inputs.attention_mask
        
        # 3. è·å– LLM åŸå§‹ Embedding
        inputs_embeds = self.llm.get_input_embeddings()(input_ids)
        
        # 4. [æ ¸å¿ƒ] æ›¿æ¢ Embedding
        # æ‰¾åˆ° input_ids ä¸­ç­‰äº <REP> çš„ä½ç½®ï¼Œæ›¿æ¢ä¸º batch ä¸­çš„ embedding
        # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾ batch['input1_emb'] å·²ç»æ˜¯ [Batch, Hidden_Dim] (å³æŠ•å½±åçš„)
        
        # ç®€å•å®ç°ï¼šå‡è®¾æ¯ä¸ª Prompt åªæœ‰ä¸€ä¸ª <REP>ï¼Œä¸”æˆ‘ä»¬ç”¨ input1_emb æ›¿æ¢
        # å¦‚æœæ˜¯åŒæ¨¡æ€ï¼Œéœ€è¦æ›´å¤æ‚çš„é€»è¾‘ (æŒ‰é¡ºåºæ›¿æ¢)
        
        rep_mask = (input_ids == self.rep_token_id)
        
        # æ£€æŸ¥ batch ä¸­æœ‰å“ªäº› embedding
        # æˆ‘ä»¬çš„ Loader äº§ç”Ÿäº† input1_emb, input2_emb ...
        embeddings_to_insert = []
        if 'input1_emb' in batch:
            embeddings_to_insert.append(batch['input1_emb'].to(device).to(inputs_embeds.dtype))
        if 'input2_emb' in batch:
            embeddings_to_insert.append(batch['input2_emb'].to(device).to(inputs_embeds.dtype))
            
        # è¿™é‡Œçš„é€»è¾‘æ˜¯å°†æ‰€æœ‰ embedding æ‹¼èµ·æ¥è¿˜æ˜¯åˆ†åˆ«æ›¿æ¢ï¼Ÿ
        # ä¸ºäº†å…¼å®¹åŸä»£ç é€»è¾‘ï¼Œé€šå¸¸æ˜¯ä¸€ä¸ªæ ·æœ¬å¯¹åº”ä¸€ä¸ªå‘é‡åºåˆ—ã€‚
        # å¦‚æœæ˜¯å•æ¨¡æ€ï¼Œinsert_emb å°±æ˜¯ [Batch, 1, 4096]
        
        if len(embeddings_to_insert) > 0:
            # æ‹¼æ¥å¤šæ¨¡æ€ (å¦‚æœéœ€è¦) æˆ–è€…åªå–ç¬¬ä¸€ä¸ª
            # ç®€åŒ–èµ·è§ï¼šæˆ‘ä»¬å‡è®¾ batch['input1_emb'] æ˜¯ä¸»è¦çš„
            insert_emb = embeddings_to_insert[0] 
            
            # ç¡®ä¿ç»´åº¦åŒ¹é…: [Batch, Seq_Len, Hidden]
            if insert_emb.dim() == 2:
                insert_emb = insert_emb.unsqueeze(1) # [Batch, 1, 4096]
            
            # æ‰§è¡Œæ›¿æ¢ (Scatter)
            # æ³¨æ„ï¼šè¿™è¦æ±‚ <REP> çš„æ•°é‡å’Œ insert_emb çš„åºåˆ—é•¿åº¦ä¸€è‡´
            # è¿™é‡Œåšä¸€ä¸ªç®€åŒ–çš„å‡è®¾ï¼šæ¯ä¸ªæ ·æœ¬åªæ›¿æ¢ä¸€ä¸ªä½ç½®
            inputs_embeds[rep_mask] = insert_emb.squeeze(1)

        # 5. Generate
        outputs = self.llm.generate(
            inputs_embeds=inputs_embeds,
            attention_mask=attention_mask,
            max_new_tokens=20,
            pad_token_id=self.tokenizer.eos_token_id,
            do_sample=False
        )
        
        # 6. Decode
        decoded = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)
        return decoded