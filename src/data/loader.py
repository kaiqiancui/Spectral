import torch
import pickle
import os
from .tasks import TaskFactory
from .dataset import UnifiedICLDataset

def load_cache(path):
    if not os.path.exists(path):
        raise FileNotFoundError(f"Cache not found: {path}")
    print(f"ğŸ§  Loading Cache: {path}")
    with open(path, "rb") as f:
        return pickle.load(f)

def apply_cache(data_list, cache, key_name):
    """å°†æ•°æ®åˆ—è¡¨ä¸­çš„æ–‡æœ¬æ›¿æ¢ä¸º Embedding"""
    valid_data = []
    miss_count = 0
    
    for item in data_list:
        text = item[key_name]
        if text in cache:
            # å¤åˆ¶ä¸€ä»½ item é˜²æ­¢ä¿®æ”¹åŸå§‹å¼•ç”¨
            new_item = item.copy()
            emb = torch.tensor(cache[text])
            # å°† embedding å­˜å…¥æ–°çš„ keyï¼Œä¾‹å¦‚ input1_emb
            new_item[f"{key_name}_emb"] = emb
            valid_data.append(new_item)
        else:
            miss_count += 1
            
    print(f"   Key: {key_name} | Hit: {len(valid_data)} | Miss: {miss_count}")
    return valid_data

def get_data_loader(cfg):
    # 1. åŠ è½½åŸå§‹æ•°æ®
    raw_splits = TaskFactory.load_raw_data(cfg.data.task_name, cfg)
    
    # 2. éå†é…ç½®ä¸­çš„ inputsï¼ŒåŠ è½½å¯¹åº”çš„ Cache å¹¶æ›¿æ¢
    # cfg.data.inputs æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼ŒåŒ…å« {column: 'input1', cache_path: '...'}
    for input_cfg in cfg.data.inputs:
        col_name = input_cfg.column
        cache_path = input_cfg.cache_path
        
        # åŠ è½½ç¼“å­˜
        cache = load_cache(cache_path)
        
        # å¯¹ Train å’Œ Test åˆ†åˆ«è¿›è¡Œæ›¿æ¢
        raw_splits['train'] = apply_cache(raw_splits['train'], cache, col_name)
        raw_splits['test'] = apply_cache(raw_splits['test'], cache, col_name)

    # 3. å°è£…æˆ PyTorch Dataset
    # è¿™é‡Œéœ€è¦ Dataset ç±»æ”¯æŒå­—å…¸åˆ—è¡¨çš„è¾“å…¥
    return {
        "train": UnifiedICLDataset(raw_splits['train'], cfg),
        "test": UnifiedICLDataset(raw_splits['test'], cfg)
    }
    
def collate_fn(batch_list):
    """
    batch_list: List[Dict], e.g., [{'input1_emb': Tensor, 'label': 1.2}, ...]
    """
    batch_out = {}
    
    # å‡è®¾æ‰€æœ‰æ ·æœ¬çš„ keys æ˜¯ä¸€æ ·çš„
    keys = batch_list[0].keys()
    
    for k in keys:
        values = [item[k] for item in batch_list]
        
        # å¦‚æœæ˜¯ Tensor (Embedding)ï¼Œstack èµ·æ¥
        if isinstance(values[0], torch.Tensor):
            batch_out[k] = torch.stack(values)
        # å¦‚æœæ˜¯æ•°å­— (Label)ï¼Œè½¬ Tensor
        elif isinstance(values[0], (int, float)):
            batch_out[k] = torch.tensor(values)
        # å¦åˆ™ (å­—ç¬¦ä¸²ç­‰)ï¼Œä¿æŒ List
        else:
            batch_out[k] = values
            
    return batch_out