import sys
import os
import torch
import numpy as np
import pandas as pd
from tqdm import tqdm
from src.data.utils.esm_utils import esm_model 
from src.data.utils.utils import (
    load_DTI, load_ESOL, load_Stability, load_Fluorescence, 
    load_Beta_Lactamase, load_ppi_affinity
)

def _find_embedding_cache(data_root, config_emb_folder=None):
    filename = "aaseq_to_rep_store.pkl"
    search_dirs = []
    if config_emb_folder: search_dirs.append(config_emb_folder)
    if data_root:
        search_dirs.append(os.path.join(data_root, "embeddings"))
        search_dirs.append(os.path.join(data_root, "cache"))
        search_dirs.append(data_root)
    
    for d in search_dirs:
        candidate = os.path.join(d, filename)
        if os.path.exists(candidate):
            # [æ ¸å¿ƒä¿®å¤] è½¬ä¸ºç»å¯¹è·¯å¾„ï¼
            abs_path = os.path.abspath(candidate)
            print(f"âœ… Auto-detected embedding cache: {abs_path}")
            return abs_path
            
    # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œä¹Ÿè¿”å›ç»å¯¹è·¯å¾„
    default_dir = os.path.join(data_root, "embeddings") if data_root else "./data/embeddings"
    os.makedirs(default_dir, exist_ok=True)
    return os.path.abspath(os.path.join(default_dir, filename))
def get_data_loader(cfg):
    task_name = cfg.data.task_name
    data_root = cfg.data.get("data_root", "./data")
    print(f"ğŸš€ Loading task: {task_name}")

    train_inputs, train_y = [], []
    test_inputs, test_y = [], []

    # --- 1. åŠ è½½åŸå§‹æ•°æ® ---
    if task_name in ['BindingDB_Ki', 'BindingDB_IC50', 'KIBA', 'DAVIS']:
        try:
            raw_data = load_DTI(
                name=task_name,
                split_method=cfg.data.get("split_method", "random"),
                max_smiles_length=cfg.data.get("max_smiles_length", None),
                max_protein_length=cfg.data.get("max_protein_length", None)
            )
            train_inputs_raw, train_y_raw, _, _, test_inputs_raw, test_y_raw = raw_data
            
            # --- [ä¿®å¤ 1] æ­£ç¡®æå– Input ---
            if hasattr(train_inputs_raw, 'iloc'):
                 train_inputs = train_inputs_raw['Target'].tolist()
                 test_inputs = test_inputs_raw['Target'].tolist()
            else:
                 train_inputs = [x[1] for x in train_inputs_raw]
                 test_inputs = [x[1] for x in test_inputs_raw]

            # --- [ä¿®å¤ 2] æ­£ç¡®æå– Label (è§£å†³ zip çŸ­æ¿é—®é¢˜) ---
            # å¦‚æœæ˜¯ DataFrame (äºŒç»´)ï¼Œè½¬ä¸º flatten çš„åˆ—è¡¨
            if hasattr(train_y_raw, 'iloc'):
                # å°è¯•å– 'Y' åˆ—ï¼Œå¦‚æœä¸è¡Œå°±å–ç¬¬ 0 åˆ—
                try:
                    train_y = train_y_raw['Y'].values.flatten().tolist()
                    test_y = test_y_raw['Y'].values.flatten().tolist()
                except KeyError:
                    train_y = train_y_raw.iloc[:, 0].values.flatten().tolist()
                    test_y = test_y_raw.iloc[:, 0].values.flatten().tolist()
            else:
                train_y = train_y_raw
                test_y = test_y_raw

        except Exception as e:
            print(f"âš ï¸ Standard load failed ({e}), trying fallback to CSV...")
            # Fallback é€»è¾‘ä¿æŒç®€åŒ–
            raise e

    elif task_name == 'ESOL':
        train_ids, train_y, _, _, test_ids, test_y = load_ESOL()
        train_inputs = train_ids.flatten().tolist()
        test_inputs = test_ids.flatten().tolist()
    else:
        # å…¶ä»–ä»»åŠ¡ç®€å•å¤„ç†
        pass 

    print(f"ğŸ“Š Raw Data Size: Train={len(train_inputs)}, Labels={len(train_y)}")
    
    # ç¡®ä¿é•¿åº¦ä¸€è‡´ï¼Œå¦åˆ™ zip ä¼šä¸¢æ•°æ®
    assert len(train_inputs) == len(train_y), f"Mismatch! Inputs: {len(train_inputs)}, Labels: {len(train_y)}"

    # --- 2. ç¼“å­˜è¿‡æ»¤ (åªä¿ç•™å‘½ä¸­ç¼“å­˜çš„æ•°æ®) ---
    print("ğŸ§¹ Filtering data by cache...")
    emb_folder_cfg = cfg.data.get("embedding_folder", None)
    cache_path = _find_embedding_cache(data_root, emb_folder_cfg)
    
    fm_model = esm_model(
        esm_model_name=cfg.data.get("esm_model_path", "facebook/esm2_t30_150M_UR50D"), 
        avoid_loading_model=False, 
        rep_cache_path=cache_path,
        use_cache=True
    )
    
    # --- [æ–°å¢] è°ƒè¯•ï¼šæ‰“å°å‰å‡ ä¸ª Key çœ‹çœ‹é•¿ä»€ä¹ˆæ · ---
    print("\nğŸ” --- DEBUG: Key Matching ---")
    cache_keys = list(fm_model.aaseq_rep_map.keys())
# ... (å‰é¢çš„ä»£ç ä¿æŒä¸å˜ï¼Œç›´åˆ° print("ğŸ” --- DEBUG: Key Matching ---") é‚£é‡Œ) ...

    # --- [å‡çº§ç‰ˆ] è°ƒè¯•ä¸è¿‡æ»¤é€»è¾‘ ---
    print("\nğŸ” --- DEBUG: Deep Inspection ---")
    
    # 1. æå–å­—å…¸é‡Œçš„ä»»æ„ä¸€ä¸ª Key è¿›è¡Œè§£å‰–
    cache_sample = list(fm_model.aaseq_rep_map.keys())[0] if fm_model.aaseq_rep_map else "EMPTY"
    input_sample = train_inputs[0] if train_inputs else "EMPTY"
    
    print(f"1. Cache Key Sample (Raw): '{cache_sample}'")
    print(f"   Length: {len(cache_sample)}")
    print(f"2. Input Data Sample (Raw): '{input_sample}'")
    print(f"   Length: {len(input_sample)}")
    
    # 3. å°è¯•æš´åŠ›åŒ¹é…æµ‹è¯•
    print("3. Running heuristic check...")
    # ç§»é™¤æ‰€æœ‰éå­—æ¯å­—ç¬¦ (ç©ºæ ¼ã€æ¢è¡Œã€åˆ¶è¡¨ç¬¦)
    import re
    clean_cache = re.sub(r'[^A-Z]', '', str(cache_sample).upper())
    clean_input = re.sub(r'[^A-Z]', '', str(input_sample).upper())
    
    print(f"   Cleaned Cache: '{clean_cache[:50]}...'")
    print(f"   Cleaned Input: '{clean_input[:50]}...'")
    
    if clean_cache == clean_input:
        print("   âœ… MATCH FOUND after cleaning! (Format mismatch detected)")
    else:
        print("   âŒ NO MATCH even after cleaning. (Datasets might be different)")
    print("---------------------------------\n")

    def normalize_seq(s):
        """å¼ºåŠ›æ¸…æ´—å‡½æ•°ï¼šè½¬å¤§å†™ï¼Œå»ç©ºæ ¼ï¼Œå»æ¢è¡Œ"""
        if not isinstance(s, str): s = str(s)
        # å¾ˆå¤šç”Ÿç‰©åºåˆ—æ–‡ä»¶ä¼šæœ‰æ¢è¡Œç¬¦æˆ–ç©ºæ ¼ï¼Œå¿…é¡»å»æ‰
        return "".join(s.split()).upper()

    def filter_and_retrieve(inputs, labels, model, desc="Filtering"):
        valid_embs = []
        valid_texts = []
        valid_labels = []
        miss_count = 0
        
        # ä¸ºäº†åŠ é€Ÿï¼Œå…ˆæŠŠ Cache çš„ Key ä¹Ÿå…¨éƒ¨æ¸…æ´—ä¸€éå¹¶å»ºç«‹æ˜ å°„
        # æ³¨æ„ï¼šè¿™ä¼šæ¶ˆè€—ä¸€äº›å†…å­˜ï¼Œä½†ä¸ºäº†åŒ¹é…æ˜¯å€¼å¾—çš„
        print(f"[{desc}] Pre-processing cache keys for robust matching...")
        # åŸå§‹Key -> Embedding
        raw_cache = model.aaseq_rep_map
        # æ¸…æ´—åKey -> åŸå§‹Key (ç”¨äºå–å€¼)
        # åªæœ‰å½“ä¸¤ä¸ªä¸åŒçš„åŸå§‹Keyæ¸…æ´—åå˜æˆåŒä¸€ä¸ªæ—¶ä¼šæœ‰å†²çªï¼Œè¿™é‡Œæš‚ä¸”å¿½ç•¥
        clean_cache_map = {normalize_seq(k): k for k in raw_cache.keys()}
        
        print(f"[{desc}] Start matching...")
        for seq, label in zip(tqdm(inputs, desc=desc), labels):
            # æ¸…æ´—è¾“å…¥
            clean_seq = normalize_seq(seq)
            
            if clean_seq in clean_cache_map:
                # å‘½ä¸­ï¼é€šè¿‡æ¸…æ´—åçš„Keyæ‰¾åˆ°åŸå§‹Keyï¼Œå†å–Embedding
                original_key = clean_cache_map[clean_seq]
                emb = raw_cache[original_key]
                
                # æ ¼å¼è½¬æ¢
                if isinstance(emb, torch.Tensor):
                    emb = emb.detach().cpu()
                elif isinstance(emb, np.ndarray):
                    emb = torch.from_numpy(emb)
                elif isinstance(emb, list):
                    emb = torch.tensor(emb)
                
                if emb.dim() == 2 and emb.shape[0] == 1:
                    emb = emb.squeeze(0)
                    
                valid_embs.append(emb)
                valid_texts.append(original_key) # å­˜åŸå§‹çš„è¿˜æ˜¯æ¸…æ´—åçš„éƒ½å¯ä»¥
                valid_labels.append(label)
            else:
                miss_count += 1
                
        print(f"   ğŸ‘‰ [{desc}] Kept: {len(valid_embs)} | Dropped: {miss_count}")
        
        if len(valid_embs) == 0:
            # æ­¤æ—¶å¦‚æœè¿˜æ˜¯0ï¼Œé‚£å°±æ˜¯çœŸæ²¡æ•°æ®äº†
            print(f"âš ï¸ WARNING: Zero matches for {desc}. This is critical.")
            # ä¸ºäº†é˜²æ­¢ç¨‹åºç›´æ¥å´©æ‰æ— æ³•çœ‹æ—¥å¿—ï¼Œæˆ‘ä»¬è¿™é‡Œå¦‚æœä¸æŠ›é”™ï¼Œåç»­ä¹Ÿä¼šæŠ¥é”™
            # ä½†æ—¢ç„¶ä½ æ˜¯åš case studyï¼Œå¯èƒ½åªè¦ä¸€éƒ¨åˆ†æ•°æ®å°±è¡Œ
            # å¦‚æœè®­ç»ƒé›†ç©ºäº†ï¼Œå¿…é¡»æŠ¥é”™
            if desc == "Train":
                raise ValueError(f"CRITICAL: No data found in cache for {desc}! Check 'Deep Inspection' logs above.")

        # å¦‚æœæ²¡æœ‰æ•°æ®ï¼Œè¿”å›ç©º tensor
        if len(valid_embs) == 0:
            return [], torch.tensor([]), np.array([])
            
        return valid_texts, torch.stack(valid_embs), np.array(valid_labels)

    train_inputs, train_emb, train_y = filter_and_retrieve(train_inputs, train_y, fm_model, "Train")
    test_inputs, test_emb, test_y = filter_and_retrieve(test_inputs, test_y, fm_model, "Test")

    return {
        "train": {"text": train_inputs, "emb": train_emb, "label": train_y},
        "test": {"text": test_inputs, "emb": test_emb, "label": test_y}
    }